{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "general-bankruptcy",
   "metadata": {},
   "source": [
    "# Deep Learning Homework\n",
    "Authors: Gergály Anna, Mészáros Péter\n",
    "\n",
    "## Downloading the datasets\n",
    "\n",
    "Insert your Kaggle API keys, to download the datasets with the Kaggle API.\n",
    "The first block sets the environment variables for the Kaggle API to work.\n",
    "The second block downloads the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files('asdasdasasdas/garbage-classification', path=\"./garbage1\", quiet=False, unzip=True)\n",
    "api.dataset_download_files('mostafaabla/garbage-classification', path=\"./garbage2\", quiet=False, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "short-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_directory='garbage1/Garbage classification/Garbage classification/'\n",
    "dataset2_directory='garbage2/garbage_classification/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-inspector",
   "metadata": {},
   "source": [
    "Removing the last unnecessary classes from the second dataset, which can't be found in the first dataset.\n",
    "Merging white-glass, brown-glass, green-glass classes into one class, named glass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "removable_classes=['battery', 'clothes', 'biological', 'shoes']\n",
    "for label in removable_classes:\n",
    "    shutil.rmtree(os.path.join(dataset2_directory,label))\n",
    "os.rename('garbage2/garbage_classification/brown-glass', 'garbage2/garbage_classification/glass')\n",
    "glasses = [os.path.join(dataset2_directory,'white-glass'), os.path.join(dataset2_directory,'green-glass')]\n",
    "for glass_directory_name in glasses:\n",
    "    for filename in os.listdir(glass_directory_name):\n",
    "        shutil.move(os.path.join(glass_directory_name, filename), os.path.join('garbage2/garbage_classification/glass', filename))\n",
    "    os.rmdir(glass_directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-penetration",
   "metadata": {},
   "source": [
    "Importing libraries and setting parameter variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "determined-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class_names=['glass', 'paper', 'cardboard', 'trash', 'metal', 'plastic']\n",
    "image_size=(256, 256)\n",
    "validation_split=0.2\n",
    "seed=111\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-hughes",
   "metadata": {},
   "source": [
    "## Reading the datasets.\n",
    "The datasets are split for training and validation in a 4:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = keras.utils.image_dataset_from_directory(\n",
    "    dataset1_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    seed=seed,\n",
    "    subset='training'\n",
    ")\n",
    "val_1 = keras.utils.image_dataset_from_directory(\n",
    "    dataset1_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    seed=seed,\n",
    "    subset='validation'\n",
    ")\n",
    "    \n",
    "train_2 = keras.utils.image_dataset_from_directory(\n",
    "    dataset2_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    seed=seed,\n",
    "    subset='training'\n",
    ")\n",
    "val_2 = keras.utils.image_dataset_from_directory(\n",
    "    dataset2_directory,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    seed=seed,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-session",
   "metadata": {},
   "source": [
    "Normalizing the images, and concatenating the 2 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "responsible-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_1 = train_1.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_1 = val_1.map(lambda x, y: (normalization_layer(x), y))\n",
    "train_2 = train_2.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_2 = val_2.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mineral-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_1.concatenate(train_2)\n",
    "val = val_1.concatenate(val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = np.concatenate([y for x, y in train], axis=0)\n",
    "x_train = np.empty(shape=(0,256,256,3))\n",
    "for image, label in train:\n",
    "    x_train = np.concatenate((x_train, image.numpy()), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
