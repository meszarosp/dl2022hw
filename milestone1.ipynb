{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oriented-henry",
   "metadata": {},
   "source": [
    "# Deep Learning Homework: Waste classification\n",
    "Authors: Gergály Anna, Mészáros Péter\n",
    "\n",
    "## Downloading the datasets\n",
    "\n",
    "Insert your Kaggle API keys, to download the datasets with the Kaggle API.\n",
    "The first block sets the environment variables for the Kaggle API to work. More info about creating a Kaggle API Token can be found here: https://www.kaggle.com/docs/api.\n",
    "The second block downloads the datasets from Kaggle.\n",
    "The third block downloads a third dataset from github as a zip file, and then extracts it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = '' #insert your api token data here\n",
    "os.environ['KAGGLE_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files('asdasdasasdas/garbage-classification', path=\"./garbage1\", quiet=False, unzip=True)\n",
    "api.dataset_download_files('mostafaabla/garbage-classification', path=\"./garbage2\", quiet=False, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.download(\"https://github.com/nikhilvenkatkumsetty/TrashBox/archive/refs/heads/main.zip\", out=\"./garbage3.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"garbage3.zip\", mode='r') as z:\n",
    "    z.extractall(\"./garbage3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_directory='garbage1/Garbage classification/Garbage classification/'\n",
    "dataset2_directory='garbage2/garbage_classification/'\n",
    "dataset3_directory='garbage3/TrashBox-main/TrashBox_train_dataset_subfolders'\n",
    "dataset_directories = [dataset1_directory, dataset2_directory, dataset3_directory]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-michael",
   "metadata": {},
   "source": [
    "Removing the last unnecessary classes from the second dataset, which can't be found in the first dataset.\n",
    "Merging white-glass, brown-glass, green-glass classes into one class, named glass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "removable_classes=['battery', 'clothes', 'biological', 'shoes']\n",
    "for label in removable_classes:\n",
    "    shutil.rmtree(os.path.join(dataset2_directory,label))\n",
    "os.rename('garbage2/garbage_classification/brown-glass', 'garbage2/garbage_classification/glass')\n",
    "glasses = [os.path.join(dataset2_directory,'white-glass'), os.path.join(dataset2_directory,'green-glass')]\n",
    "for glass_directory_name in glasses:\n",
    "    for filename in os.listdir(glass_directory_name):\n",
    "        shutil.move(os.path.join(glass_directory_name, filename), os.path.join('garbage2/garbage_classification/glass', filename))\n",
    "    os.rmdir(glass_directory_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-defensive",
   "metadata": {},
   "source": [
    "Adding the extra classes to the first and second datasets (e-waste and medical) and adding trash to the third dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(os.path.join(dataset1_directory, 'e-waste'))\n",
    "os.mkdir(os.path.join(dataset1_directory, 'medical'))\n",
    "os.mkdir(os.path.join(dataset2_directory, 'e-waste'))\n",
    "os.mkdir(os.path.join(dataset2_directory, 'medical'))\n",
    "os.mkdir(os.path.join(dataset3_directory, 'trash'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-topic",
   "metadata": {},
   "source": [
    "## Reading the datasets.\n",
    "Importing libraries and setting hyperparameter variables.\n",
    "The datasets are split for training and validation in a 4:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing import image as image_utils\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class_names=['glass', 'paper', 'cardboard', 'trash', 'metal', 'plastic', 'e-waste', 'medical']\n",
    "image_size=(256, 256)\n",
    "validation_split=0.2\n",
    "seed=111\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ = []\n",
    "val_ = []\n",
    "for i in range(3):\n",
    "    train_.append(keras.utils.image_dataset_from_directory(\n",
    "    dataset_directories[i],\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    seed=seed,\n",
    "    subset='training'\n",
    "    ))\n",
    "    val_.append(keras.utils.image_dataset_from_directory(\n",
    "    dataset_directories[i],\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    seed=seed,\n",
    "    subset='validation'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-wellington",
   "metadata": {},
   "source": [
    "Normalizing the images, and concatenating the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "for i in range(3):\n",
    "    train_[i] = train_[i].map(lambda x, y: (normalization_layer(x), y))\n",
    "    val_[i] = val_[i].map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_[0].concatenate(train_[1]).concatenate(train_[2]) #training X and Y\n",
    "val = val_[0].concatenate(val_[1]).concatenate(val_[2]) #validation X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-government",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
